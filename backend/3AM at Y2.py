# -*- coding: utf-8 -*-
"""3AM at Y2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wWc3f5kiNCoNvnUSeD34396d8tW3YMCW
"""

# -*- coding: utf-8 -*-
"""
Experiment Y-Prime: Refined 4-Watermark Parallel PFB System

This Colab notebook contains the refined architecture for the PFBGenerator and PFBDetector,
building upon Experiment X to improve imperceptibility (SNR) and robustness (BER) for
the 4-watermark parallel frequency band audio watermarking system.

Key refinements include:
1.  PFBGenerator: Introduction of learnable modulation strength *per frequency band*
    to allow for more nuanced and perceptually optimized embedding.
2.  PFBDetector: Implementation of *band-specific decoding heads* in the final
    fully-connected layers, allowing each head to specialize in decoding
    its respective watermark's bits.

This script sets up the environment, defines the enhanced models, and provides
a full training and evaluation framework using the VoxPopuli dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
# 
# %%capture
# !pip install torchaudio soundfile matplotlib scipy pandas seaborn audioseal
# !pip install -U datasets
# !pip install accelerate -U # Might be useful for faster training on GPU, though not explicitly used here.
#

# -*- coding: utf-8 -*-
"""
Experiment Y-Prime: Refined 4-Watermark Parallel PFB System

This Colab notebook contains the refined architecture for the PFBGenerator and PFBDetector,
building upon Experiment X to improve imperceptibility (SNR) and robustness (BER) for
the 4-watermark parallel frequency band audio watermarking system.

Key refinements include:
1.  PFBGenerator: Introduction of learnable modulation strength *per frequency band*
    to allow for more nuanced and perceptually optimized embedding.
2.  PFBDetector: Implementation of *band-specific decoding heads* in the final
    fully-connected layers, allowing each head to specialize in decoding
    its respective watermark's bits.

This script sets up the environment, defines the enhanced models, and provides
a full training and evaluation framework using the VoxPopuli dataset.
"""

# %%capture
# !pip install torchaudio soundfile matplotlib scipy pandas seaborn audioseal
# !pip install -U datasets
# !pip install accelerate -U # Might be useful for faster training on GPU, though not explicitly used here.

# --- 1. Imports and Setup ---
import torch
import torch.nn as nn
import torch.fft
import torchaudio
import torchaudio.transforms as T
import torchaudio.functional as F
import torch.nn.functional as torch_F
import numpy as np
from torch.utils.data import Dataset, DataLoader, IterableDataset
import random
from datasets import load_dataset, VerificationMode
import sys
import math
from torch.optim.lr_scheduler import ReduceLROnPlateau
from google.colab import drive
import os
from tqdm.notebook import tqdm # For progress bars in Colab

# --- 2. Constants for Experiment Y-Prime ---
print("Defining constants for Experiment Y-Prime...")

SAMPLE_RATE = 16000
N_FFT = 512
HOP_LENGTH = N_FFT // 4 # 128
NUM_FREQ_BANDS = 4      # The number of parallel watermarks
N_BITS = 8              # Number of bits per watermark message (per band)
SEGMENT_DURATION_S = 1  # Audio segment duration in seconds
NUM_SAMPLES_SEGMENT = SAMPLE_RATE * SEGMENT_DURATION_S # 16000 samples per segment
NUM_TIME_FRAMES_SEGMENT = (NUM_SAMPLES_SEGMENT // HOP_LENGTH) + 1 # 126
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model architecture and training parameters
# Note: DETECTOR_FC_HIDDEN_SIZE_X is from previous exp, we multiply it for Y-Prime
DETECTOR_FC_HIDDEN_SIZE_Y_PRIME = 192 * 2 # Refinement: Increased hidden size for detector
BATCH_SIZE = 16 # Used for both training and evaluation
NUM_EPOCHS = 200 # Recommended number of epochs for training

# Loss weights (these will need careful tuning during actual training)
LAMBDA_IMPERCEPT_TIME = 10.0  # Weight for time-domain imperceptibility (MSE)
LAMBDA_IMPERCEPT_FREQ = 1.0   # Weight for frequency-domain imperceptibility (L1)
LAMBDA_DETECTION_G = 5.0      # Weight for generator's detection loss (BCE)

# Detector loss weights (for BCE on detector's prediction)
# These are typically simpler as detector just wants to be accurate
LAMBDA_DETECTOR_ACCURACY = 1.0

# --- 3. Helper Functions (Adapted/Copied) ---
def add_gaussian_noise(waveform_batch, snr_db_min, snr_db_max, device=DEVICE):
    """Adds Gaussian noise to a batch of waveforms."""
    waveform_batch = waveform_batch.to(device)
    batch_size, channels, num_samples = waveform_batch.shape
    noisy_waveforms = torch.zeros_like(waveform_batch)
    epsilon_power = 1e-12

    for i in range(batch_size):
        waveform = waveform_batch[i]
        signal_power = torch.mean(waveform**2)

        if signal_power < epsilon_power:
            noisy_waveforms[i] = waveform.clone()
            continue

        snr_db = random.uniform(snr_db_min, snr_db_max)
        snr_linear = 10**(snr_db / 10)
        noise_power_target = signal_power / (snr_linear + epsilon_power)
        noise_std = torch.sqrt(noise_power_target)

        if torch.isnan(noise_std) or torch.isinf(noise_std) or noise_std < 0:
            noise_std = torch.tensor(0.0, device=device)

        noise = torch.randn_like(waveform) * noise_std
        noisy_waveforms[i] = waveform + noise

    return noisy_waveforms

def apply_low_pass_filter(waveform_batch, cutoff_freq_min, cutoff_freq_max, sample_rate=SAMPLE_RATE, device=DEVICE):
    """Applies a low-pass biquad filter to a batch of waveforms."""
    waveform_batch = waveform_batch.to(device)
    transformed_audios = torch.zeros_like(waveform_batch)

    for i in range(waveform_batch.shape[0]):
        waveform_single = waveform_batch[i:i+1]
        cutoff_freq = random.uniform(cutoff_freq_min, cutoff_freq_max)
        nyquist = sample_rate / 2
        actual_cutoff = min(cutoff_freq, nyquist - 1)

        if actual_cutoff <= 0:
            transformed_audios[i] = waveform_single.clone() # Use clone to avoid modifying original
            continue

        # Adjust for torchaudio's expected input dimensions
        if waveform_single.dim() == 3 and waveform_single.shape[1] == 1:
            transformed_audios[i] = F.lowpass_biquad(waveform_single.squeeze(1), sample_rate, actual_cutoff).unsqueeze(1)
        elif waveform_single.dim() == 2:
            transformed_audios[i] = F.lowpass_biquad(waveform_single, sample_rate, actual_cutoff)
        elif waveform_single.dim() == 1: # Handle 1D inputs if they somehow occur
            transformed_audios[i] = F.lowpass_biquad(waveform_single, sample_rate, actual_cutoff).unsqueeze(0)
        else: # Fallback or error if unexpected dimensions
            transformed_audios[i] = waveform_single.clone()
            print(f"Warning: Unexpected waveform_single dim for LPF: {waveform_single.dim()}")
    return transformed_audios

def calculate_snr(original, processed, epsilon=1e-9):
    """Calculates SNR in dB for a batch of audio signals."""
    original = original.to(DEVICE)
    processed = processed.to(DEVICE)
    noise = processed - original

    signal_power_per_item = torch.mean(original**2, dim=list(range(1, original.dim())))
    noise_power_per_item = torch.mean(noise**2, dim=list(range(1, noise.dim())))

    snr_vals_per_item = 10 * torch.log10(signal_power_per_item / (noise_power_per_item + epsilon))
    snr_vals_per_item[noise_power_per_item <= epsilon] = 100.0 # Assign high SNR if noise is practically zero

    return snr_vals_per_item.mean().item()

# Spectrogram transforms (shared by Generator and Detector)
stft_transform = T.Spectrogram(n_fft=N_FFT, hop_length=HOP_LENGTH, power=None, return_complex=True, center=True).to(DEVICE)
istft_transform = T.InverseSpectrogram(n_fft=N_FFT, hop_length=HOP_LENGTH).to(DEVICE)

# --- Dataset Class for VoxPopuli ---
class VoxPopuliSegmentDataset(Dataset):
    """
    Custom Dataset for loading VoxPopuli audio segments.
    Segments audio into fixed-duration chunks and provides them for training.
    """
    def __init__(self, dataset_name="facebook/voxpopuli", split="train", num_samples=NUM_SAMPLES_SEGMENT, sample_rate=SAMPLE_RATE, num_examples_to_load=500):
        print(f"Loading VoxPopuli dataset (split: {split}). This may take a moment...")
        # FIX: Re-enabling streaming=True and directly taking items from the iterator to avoid full dataset download.
        self.dataset = load_dataset(dataset_name, "en", split=split, verification_mode=VerificationMode.NO_CHECKS, streaming=True)

        self.num_samples = num_samples
        self.sample_rate = sample_rate
        self.max_segments_per_audio = 1 # Only take 1 segment per audio for simplicity in this example

        self.processed_data = []
        print(f"Preprocessing {num_examples_to_load} audio segments...")

        # Iterate over the streaming dataset. `take` is handled by the iterator.
        # This prevents the full dataset from being downloaded to disk.
        for i, item in enumerate(tqdm(self.dataset, total=num_examples_to_load, desc="Loading Audio Segments")):
            if i >= num_examples_to_load:
                break # Stop after loading desired number of examples

            audio = item['audio']
            # Ensure waveform is a torch tensor with a channel dimension
            waveform = torch.tensor(audio['array'], dtype=torch.float32).unsqueeze(0)
            current_sample_rate = audio['sampling_rate']

            if current_sample_rate != self.sample_rate:
                # Resample if needed
                resampler = T.Resample(current_sample_rate, self.sample_rate).to(DEVICE)
                waveform = resampler(waveform.to(DEVICE)).cpu() # Resample on device, then move back to CPU for storage

            # Segment into fixed chunks
            num_segments = waveform.shape[-1] // self.num_samples
            for j in range(min(num_segments, self.max_segments_per_audio)):
                segment = waveform[:, j * self.num_samples : (j + 1) * self.num_samples]
                # Normalize segment to a reasonable range (e.g., -1 to 1)
                segment_max_abs = torch.max(torch.abs(segment))
                segment = segment / segment_max_abs if segment_max_abs > 1e-6 else segment
                self.processed_data.append(segment)

        # Fallback for empty dataset
        if not self.processed_data:
            print("Warning: No audio segments loaded from dataset. Generating dummy data for demonstration.")
            self.processed_data = [torch.randn(1, self.num_samples) for _ in range(num_examples_to_load)]

        print(f"Finished preprocessing. Total segments: {len(self.processed_data)}")

    def __len__(self):
        return len(self.processed_data)

    def __getitem__(self, idx):
        return self.processed_data[idx]

# --- 4. Refined Model Class Definitions for Experiment Y-Prime ---
class PFBGenerator(nn.Module):
    """
    PFBGenerator (Experiment Y-Prime Refined Version)

    This generator embeds multiple watermarks into distinct frequency bands.
    Refinements for Experiment Y-Prime:
    -   Introduces 'learnable_modulation_strengths_raw' parameter for each frequency band,
        allowing dynamic adjustment of embedding intensity per band.
    -   Applies `Tanh` to the projected deltas before scaling, ensuring modifications
        are strictly bounded and subtle for improved imperceptibility.
    -   Uses `nn.ModuleList` for `modification_projectors` to enable band-specific learning.
    """
    def __init__(self, n_fft=N_FFT, num_freq_bands=NUM_FREQ_BANDS, n_bits=N_BITS,
                 message_embedding_dim=16, initial_modulation_strength=0.3): # Lower initial strength
        super().__init__()
        self.n_fft = n_fft
        self.num_freq_bands = num_freq_bands
        self.n_bits = n_bits
        self.message_embedding_dim = message_embedding_dim

        num_freq_bins_total = self.n_fft // 2 + 1
        self.nominal_bins_per_band = num_freq_bins_total // self.num_freq_bands

        self.message_embedding = nn.Embedding(2, self.message_embedding_dim)

        # Refinement 1: Learnable modulation strength *per frequency band*
        initial_strength_param = math.log(initial_modulation_strength / (1 - initial_modulation_strength))
        self.learnable_modulation_strengths_raw = nn.Parameter(
            torch.full((self.num_freq_bands,), initial_strength_param, device=DEVICE)
        )

        # Refinement 2: Modification projector for each band (ModuleList for distinct learning)
        self.modification_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.n_bits * self.message_embedding_dim, 128),
                nn.ReLU(),
                nn.Linear(128, self.nominal_bins_per_band),
                nn.Tanh() # Ensures output deltas are between -1 and 1 for subtle modifications
            ) for _ in range(self.num_freq_bands)
        ])

        print(f"PFBGenerator (Exp Y-Prime Refined) initialized with {self.num_freq_bands} band-specific projectors.")
        print(f"Initial learnable modulation strength param (pre-sigmoid): {initial_strength_param:.4f}")

    def forward(self, audio_segment_batch, message_batches: list):
        """
        Forward pass for the PFBGenerator.
        audio_segment_batch: Batch of original audio waveforms.
        message_batches: List of message batches, one for each frequency band.
                         e.g., [batch_wm0_msg, batch_wm1_msg, batch_wm2_msg, batch_wm3_msg]
        """
        if len(message_batches) != self.num_freq_bands:
            raise ValueError(f"Expected {self.num_freq_bands} message batches, got {len(message_batches)}")

        if audio_segment_batch.dim() == 2:
            audio_segment_batch = audio_segment_batch.unsqueeze(1)

        audio_segment_batch_device = audio_segment_batch.to(DEVICE)

        spectrogram_complex = stft_transform(audio_segment_batch_device.squeeze(1))
        magnitude = spectrogram_complex.abs()
        angle = spectrogram_complex.angle()

        modified_band_magnitudes = []
        num_freq_bins_total_runtime = magnitude.shape[1]

        # Calculate actual modulation strengths from learnable parameters (sigmoid to keep between 0 and 1)
        modulation_strengths = torch.sigmoid(self.learnable_modulation_strengths_raw)

        for i in range(self.num_freq_bands):
            message_batch = message_batches[i]
            embedded_message_flat = self.message_embedding(message_batch.long()).view(message_batch.size(0), -1)

            # Get deltas from band-specific projector
            projected_deltas_raw = self.modification_projectors[i](embedded_message_flat)

            start_bin = i * self.nominal_bins_per_band
            if i == self.num_freq_bands - 1:
                end_bin = num_freq_bins_total_runtime
            else:
                end_bin = start_bin + self.nominal_bins_per_band

            current_band_actual_size = end_bin - start_bin
            num_deltas_to_apply = min(projected_deltas_raw.shape[1], current_band_actual_size)

            full_deltas_for_band_slice = torch.zeros(message_batch.size(0), current_band_actual_size, device=DEVICE)
            full_deltas_for_band_slice[:, :num_deltas_to_apply] = projected_deltas_raw[:, :num_deltas_to_apply]

            # Apply learnable modulation strength per band to the Tanh-bounded deltas
            # FIX: Convert modulation_strengths[i] to a scalar using .item() for clamp's max argument
            max_clamp_val = 1.0 + modulation_strengths[i].item() # .item() converts scalar tensor to Python float
            scaling_factors = (1.0 + full_deltas_for_band_slice * modulation_strengths[i]).unsqueeze(-1)
            # Ensure magnitudes remain positive and bounded. min=0.01 prevents division by zero later.
            scaling_factors = torch.clamp(scaling_factors, min=0.01, max=max_clamp_val)

            original_band_slice = magnitude[:, start_bin:end_bin, :]
            modified_slice = original_band_slice * scaling_factors
            modified_band_magnitudes.append(modified_slice)

        modified_magnitude = torch.cat(modified_band_magnitudes, dim=1)

        watermarked_spectrogram_complex = torch.polar(modified_magnitude, angle)
        watermarked_audio_segment = istft_transform(watermarked_spectrogram_complex, length=audio_segment_batch_device.shape[-1])
        if watermarked_audio_segment.dim() == 2:
            watermarked_audio_segment = watermarked_audio_segment.unsqueeze(1)

        return watermarked_audio_segment

class PFBDetector(nn.Module):
    """
    PFBDetector (Experiment Y-Prime Refined Version)

    This detector extracts watermarks from distinct frequency bands.
    Refinements for Experiment Y-Prime:
    -   Implements 'band-specific decoding heads' where each frequency band's
        processed features are fed into a dedicated final linear layer to predict
        its corresponding message bits. This allows for specialization.
    -   Increased hidden layer size for the FC network for potentially better feature learning.
    -   Enhanced convolutional backbone.
    """
    def __init__(self, n_fft=N_FFT, num_freq_bands=NUM_FREQ_BANDS, n_bits=N_BITS,
                 num_time_frames=NUM_TIME_FRAMES_SEGMENT,
                 fc_hidden_size=DETECTOR_FC_HIDDEN_SIZE_Y_PRIME): # Refinement 3: Increased hidden size
        super().__init__()
        self.n_fft = n_fft
        self.num_freq_bands = num_freq_bands
        self.n_bits = n_bits
        self.num_time_frames = num_time_frames
        self.fc_hidden_size = fc_hidden_size

        num_freq_bins_total = self.n_fft // 2 + 1
        self.freq_bins_per_band_input_shape = num_freq_bins_total // self.num_freq_bands

        # Refinement 4: Shared convolutional backbone with increased depth
        self.conv_net = nn.Sequential(
            nn.Conv2d(1, 16, (3, 5), stride=(1, 2), padding=(1, 2)),
            nn.InstanceNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Conv2d(16, 32, (3, 3), stride=(1, 1), padding=(1, 1)), # Added an extra conv layer
            nn.InstanceNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Conv2d(32, 64, (3, 3), stride=(1, 1), padding=(1, 1)), # Another extra conv layer
            nn.InstanceNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d((2, 2))
        )

        # Calculate the output size of the conv_net to properly initialize the FC layers
        with torch.no_grad():
            is_training = self.conv_net.training
            self.conv_net.eval()
            dummy_input = torch.randn(1, 1, self.freq_bins_per_band_input_shape, self.num_time_frames)
            dummy_out = self.conv_net(dummy_input)
            self.conv_output_size = dummy_out.view(1, -1).size(1)
            if is_training:
                self.conv_net.train()

        # Refinement 5: Band-specific decoding heads (separate FC networks for each band)
        self.fc_nets = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.conv_output_size, self.fc_hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(self.fc_hidden_size, self.n_bits) # Output logits for N_BITS
            ) for _ in range(self.num_freq_bands)
        ])

        print(f"PFBDetector (Exp Y-Prime Refined) initialized with {self.num_freq_bands} band-specific heads.")
        print(f"FC hidden size: {self.fc_hidden_size}, Conv output size: {self.conv_output_size}")

    def _process_band(self, magnitude_diff_slice, band_idx):
        """
        Helper to process a single band's magnitude difference slice through the conv_net
        and then its specific FC head.
        """
        # Pad/truncate frequency bins to match expected input shape for conv_net
        current_band_freq_size_actual = magnitude_diff_slice.shape[1]
        if current_band_freq_size_actual < self.freq_bins_per_band_input_shape:
            processed_band_slice = torch_F.pad(magnitude_diff_slice, (0, 0, 0, self.freq_bins_per_band_input_shape - current_band_freq_size_actual))
        elif current_band_freq_size_actual > self.freq_bins_per_band_input_shape:
            processed_band_slice = magnitude_diff_slice[:, :self.freq_bins_per_band_input_shape, :]
        else:
            processed_band_slice = magnitude_diff_slice

        # Pad/truncate time frames to match expected input shape for conv_net
        current_band_time_size_actual = processed_band_slice.shape[2]
        if current_band_time_size_actual < self.num_time_frames:
            processed_band_slice = torch_F.pad(processed_band_slice, (0, self.num_time_frames - current_band_time_size_actual, 0, 0))
        elif current_band_time_size_actual > self.num_time_frames:
            processed_band_slice = processed_band_slice[:, :, :self.num_time_frames]

        conv_input = processed_band_slice.unsqueeze(1) # Add channel dimension (batch, 1, H, W)
        conv_output = self.conv_net(conv_input)

        conv_output_flat = conv_output.view(conv_output.size(0), -1)

        band_logits = self.fc_nets[band_idx](conv_output_flat)
        return band_logits

    def forward(self, original_audio_segment_batch, watermarked_audio_segment_batch):
        """
        Forward pass for the PFBDetector.
        Processes the magnitude difference between original and watermarked audio
        for each frequency band and predicts message bits for each.
        """
        if original_audio_segment_batch.dim() == 2:
            original_audio_segment_batch = original_audio_segment_batch.unsqueeze(1)
        if watermarked_audio_segment_batch.dim() == 2:
            watermarked_audio_segment_batch = watermarked_audio_segment_batch.unsqueeze(1)

        original_audio_device = original_audio_segment_batch.to(DEVICE)
        watermarked_audio_device = watermarked_audio_segment_batch.to(DEVICE)

        spec_complex_orig = stft_transform(original_audio_device.squeeze(1))
        spec_complex_wm = stft_transform(watermarked_audio_device.squeeze(1))

        magnitude_orig = spec_complex_orig.abs()
        magnitude_wm = spec_complex_wm.abs()

        # The difference magnitude for the detector
        magnitude_diff = magnitude_wm - magnitude_orig # As per your original Experiment X code

        num_freq_bins_total_runtime = magnitude_diff.shape[1]
        bins_per_band_runtime_nominal = num_freq_bins_total_runtime // self.num_freq_bands

        all_logits = []
        for i in range(self.num_freq_bands):
            start_bin = i * bins_per_band_runtime_nominal
            if i == self.num_freq_bands - 1:
                end_bin = num_freq_bins_total_runtime
            else:
                end_bin = start_bin + bins_per_band_runtime_nominal

            target_band_diff_slice = magnitude_diff[:, start_bin:end_bin, :]
            band_logits = self._process_band(target_band_diff_slice, i) # Pass band index
            all_logits.append(band_logits)

        return all_logits

# --- 5. Robustness Evaluation Function (from Experiment X, slightly adapted) ---
def evaluate_robustness_parallel_pfb(generator, detector, num_eval_batches=50, batch_size_eval=BATCH_SIZE, device=DEVICE, dataset_loader=None):
    """
    Evaluates the robustness of the parallel PFB watermarking system.
    Calculates BER per band, overall average BER, and SNR after various attacks.
    Can optionally use a provided dataset_loader for evaluation data.
    """
    generator.eval()
    detector.eval()
    attack_results = {}
    epsilon_norm_eval = 1e-7

    attacks = {
        "no_attack": lambda x: x,
        "gaussian_noise_snr20": lambda x: add_gaussian_noise(x, 20, 20, device),
        "gaussian_noise_snr10": lambda x: add_gaussian_noise(x, 10, 10, device),
        "low_pass_filter_3kHz": lambda x: apply_low_pass_filter(x, 3000, 3000, SAMPLE_RATE, device),
        "low_pass_filter_5kHz": lambda x: apply_low_pass_filter(x, 5000, 5000, SAMPLE_RATE, device),
    }

    print("\n--- Starting Robustness Evaluation (4-WM Parallel PFB) ---")

    # Use dataset_loader if provided, otherwise generate random audio
    if dataset_loader:
        eval_data_iterator = iter(dataset_loader)
    else:
        print("Warning: No dataset_loader provided for evaluation. Generating random audio for evaluation.")

    for attack_name, attack_fn in attacks.items():
        print(f"Evaluating attack: {attack_name}...")

        total_ber_per_band = [0] * detector.num_freq_bands
        total_snr = 0
        num_samples = 0
        num_total_bits_processed = 0

        # Create a new tqdm progress bar for each attack
        with tqdm(total=num_eval_batches, desc=f"Attack: {attack_name}") as pbar:
            for _ in range(num_eval_batches):
                try:
                    original_audio_raw = next(eval_data_iterator).to(device)
                except (StopIteration, TypeError):
                    # If dataset_loader exhausted or not provided, generate random data
                    original_audio_raw = torch.randn(batch_size_eval, 1, NUM_SAMPLES_SEGMENT, device=device)
                    # For eval, ensure it's on device and correct dim
                    if original_audio_raw.dim() == 2: original_audio_raw = original_audio_raw.unsqueeze(1)


                max_vals_orig = torch.max(torch.abs(original_audio_raw), dim=-1, keepdim=True)[0] + epsilon_norm_eval
                original_audio_norm = original_audio_raw / max_vals_orig

                message_batches_eval = [
                    torch.randint(0, 2, (batch_size_eval, N_BITS), device=device, dtype=torch.float32)
                    for _ in range(detector.num_freq_bands)
                ]

                with torch.no_grad():
                    wm_audio_raw = generator(original_audio_norm, message_batches_eval)
                    max_vals_wm = torch.max(torch.abs(wm_audio_raw), dim=-1, keepdim=True)[0] + epsilon_norm_eval
                    wm_audio_norm = wm_audio_raw / max_vals_wm

                    attacked_audio = attack_fn(wm_audio_norm.clone())
                    max_vals_att = torch.max(torch.abs(attacked_audio), dim=-1, keepdim=True)[0] + epsilon_norm_eval
                    attacked_audio_norm = attacked_audio / max_vals_att

                    logits_list = detector(original_audio_norm, attacked_audio_norm)

                    for i in range(detector.num_freq_bands):
                        preds = (torch.sigmoid(logits_list[i]) > 0.5).int()
                        total_ber_per_band[i] += (message_batches_eval[i].int() != preds).float().sum().item()

                    num_total_bits_processed += message_batches_eval[0].numel()

                    current_batch_snr = calculate_snr(original_audio_norm, attacked_audio_norm)
                    total_snr += current_batch_snr * original_audio_batch.size(0) # Corrected this line to use original_audio_batch.size(0)
                    num_samples += original_audio_batch.size(0)
                pbar.update(1) # Update progress bar

        avg_ber_per_band_val = [ber / num_total_bits_processed if num_total_bits_processed > 0 else 1.0 for ber in total_ber_per_band]
        avg_ber_overall_val = sum(avg_ber_per_band_val) / detector.num_freq_bands if detector.num_freq_bands > 0 else 0.0
        avg_snr_after_attack = total_snr / num_samples if num_samples > 0 else 0.0

        attack_results[attack_name] = {
            "ber_per_band": avg_ber_per_band_val,
            "ber_avg": avg_ber_overall_val,
            "snr_after_attack": avg_snr_after_attack
        }

        ber_bands_str = "/".join([f"{b:.4f}" for b in avg_ber_per_band_val])
        print(f" BER_Avg: {avg_ber_overall_val:.4f}, BERs (Bands 0/1/2/3): {ber_bands_str}, SNR: {avg_snr_after_attack:.2f} dB")
    return attack_results

# --- 6. Main Execution Block for Training and Evaluation ---
if __name__ == '__main__':
    print("Starting Experiment Y-Prime (Refined 4-Watermark PFB) script.")

    # Mount Google Drive
    try:
        drive.mount('/content/drive', force_remount=True)
        checkpoint_dir_y_prime = "/content/drive/MyDrive/ColabNotebooks/AudioWatermarkingExpY_PFB_Parallel_4WM_Refined/"
        os.makedirs(checkpoint_dir_y_prime, exist_ok=True)
        best_chk_path_y_prime = os.path.join(checkpoint_dir_y_prime, "pfb_parallel_4wm_exp_y_prime_best.pth")
    except Exception as e:
        print(f"Drive mount or directory creation failed: {e}. Cannot proceed. Running without saving/loading models.")
        best_chk_path_y_prime = None # Disable checkpointing if drive fails

    # Instantiate refined models
    generator = PFBGenerator(initial_modulation_strength=0.3).to(DEVICE)
    detector = PFBDetector(fc_hidden_size=DETECTOR_FC_HIDDEN_SIZE_Y_PRIME).to(DEVICE)
    print("\n--- Refined PFB Generator & Detector Initialized ---")

    # Define Optimizers
    optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)
    optimizer_d = torch.optim.Adam(detector.parameters(), lr=1e-4)

    # Define Loss Functions
    criterion_bce = nn.BCEWithLogitsLoss() # For detection loss (generator and detector)
    criterion_mse = nn.MSELoss()           # For time-domain imperceptibility
    criterion_l1 = nn.L1Loss()             # For frequency-domain imperceptibility

    # --- Dataset and DataLoader ---
    # Load dataset for training and evaluation
    train_dataset = VoxPopuliSegmentDataset(split="train", num_samples=NUM_SAMPLES_SEGMENT, sample_rate=SAMPLE_RATE, num_examples_to_load=2000) # Increased to 2000 for more data
    val_dataset = VoxPopuliSegmentDataset(split="validation", num_samples=NUM_SAMPLES_SEGMENT, sample_rate=SAMPLE_RATE, num_examples_to_load=500) # 500 for validation

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)

    print(f"Train loader has {len(train_loader)} batches. Val loader has {len(val_loader)} batches.")

    best_val_ber = float('inf') # Track best validation BER to save model

    # --- Training Loop ---
    print("\n--- Starting Training for Experiment Y-Prime ---")
    for epoch in range(NUM_EPOCHS):
        generator.train()
        detector.train()
        total_gen_loss = 0
        total_det_loss = 0
        train_ber = 0
        num_train_batches = 0

        # Use tqdm for a clear progress bar during training
        for batch_idx, original_audio_batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} (Train)")):
            original_audio_batch = original_audio_batch.to(DEVICE)

            # Generate random messages for each band
            message_batches = [
                torch.randint(0, 2, (original_audio_batch.size(0), N_BITS), device=DEVICE, dtype=torch.float32)
                for _ in range(NUM_FREQ_BANDS)
            ]

            # --- Generator Training Step ---
            optimizer_g.zero_grad()
            watermarked_audio = generator(original_audio_batch, message_batches)

            # Imperceptibility Losses
            loss_impercept_time = criterion_mse(watermarked_audio, original_audio_batch)
            loss_impercept_freq = criterion_l1(stft_transform(watermarked_audio.squeeze(1)).abs(), stft_transform(original_audio_batch.squeeze(1)).abs())

            # Detection Loss for Generator (Generator wants detector to succeed)
            det_logits_g = detector(original_audio_batch, watermarked_audio)
            # Combine logits for all bands for generator's detection loss
            loss_detection_g = sum(criterion_bce(logits, msg) for logits, msg in zip(det_logits_g, message_batches)) / NUM_FREQ_BANDS

            # Total Generator Loss (weighted)
            gen_loss = (LAMBDA_IMPERCEPT_TIME * loss_impercept_time +
                        LAMBDA_IMPERCEPT_FREQ * loss_impercept_freq +
                        LAMBDA_DETECTION_G * loss_detection_g)

            gen_loss.backward()
            optimizer_g.step()
            total_gen_loss += gen_loss.item()

            # --- Detector Training Step ---
            optimizer_d.zero_grad()
            # Generate watermarked audio again (or detach from generator's graph) for detector training
            # Detaching here ensures detector trains on a fixed watermark from current generator state
            with torch.no_grad():
                 watermarked_audio_det_input = generator(original_audio_batch, message_batches).detach()

            det_logits_d = detector(original_audio_batch, watermarked_audio_det_input)

            # Detector Loss (wants to accurately predict messages)
            det_loss = sum(criterion_bce(logits, msg) for logits, msg in zip(det_logits_d, message_batches)) / NUM_FREQ_BANDS

            det_loss.backward()
            optimizer_d.step()
            total_det_loss += det_loss.item()

            # Calculate training BER
            with torch.no_grad():
                current_train_ber = 0
                for i in range(NUM_FREQ_BANDS):
                    preds = (torch.sigmoid(det_logits_d[i]) > 0.5).int()
                    current_train_ber += (message_batches[i].int() != preds).float().sum().item()
                train_ber += current_train_ber / (original_audio_batch.size(0) * N_BITS * NUM_FREQ_BANDS)
            num_train_batches += 1

        avg_gen_loss = total_gen_loss / num_train_batches
        avg_det_loss = total_det_loss / num_train_batches
        avg_train_ber = train_ber / num_train_batches

        print(f"Epoch {epoch+1} - Gen Loss: {avg_gen_loss:.4f}, Det Loss: {avg_det_loss:.4f}, Train BER: {avg_train_ber:.4f}")

        # --- Validation Step ---
        generator.eval()
        detector.eval()
        val_ber_per_band = [0] * NUM_FREQ_BANDS
        val_total_bits = 0
        val_snr = 0
        val_num_samples = 0

        with torch.no_grad():
            for original_audio_batch_val in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} (Val)"):
                original_audio_batch_val = original_audio_batch_val.to(DEVICE)
                message_batches_val = [
                    torch.randint(0, 2, (original_audio_batch_val.size(0), N_BITS), device=DEVICE, dtype=torch.float32)
                    for _ in range(NUM_FREQ_BANDS)
                ]

                watermarked_audio_val = generator(original_audio_batch_val, message_batches_val)
                det_logits_val = detector(original_audio_batch_val, watermarked_audio_val)

                for i in range(NUM_FREQ_BANDS):
                    preds_val = (torch.sigmoid(det_logits_val[i]) > 0.5).int()
                    val_ber_per_band[i] += (message_batches_val[i].int() != preds_val).float().sum().item()
                val_total_bits += original_audio_batch_val.size(0) * N_BITS * NUM_FREQ_BANDS

                # Calculate validation SNR
                current_val_snr = calculate_snr(original_audio_batch_val, watermarked_audio_val)
                val_snr += current_val_snr * original_audio_batch_val.size(0)
                val_num_samples += original_audio_batch_val.size(0)

        avg_val_ber = sum(val_ber_per_band) / val_total_bits if val_total_bits > 0 else 1.0
        avg_val_snr = val_snr / val_num_samples if val_num_samples > 0 else 0.0

        print(f"Validation BER: {avg_val_ber:.4f}, Validation SNR: {avg_val_snr:.2f} dB")

        # Save the best model based on validation BER
        if avg_val_ber < best_val_ber:
            best_val_ber = avg_val_ber
            if best_chk_path_y_prime:
                torch.save({
                    'epoch': epoch,
                    'generator_state_dict': generator.state_dict(),
                    'detector_state_dict': detector.state_dict(),
                    'optimizer_g_state_dict': optimizer_g.state_dict(),
                    'optimizer_d_state_dict': optimizer_d.state_dict(),
                    'best_val_ber': best_val_ber,
                }, best_chk_path_y_prime)
                print(f"Model saved! Validation BER improved to {best_val_ber:.4f}")
            else:
                print("Skipping model save as checkpoint path is not valid.")

    print("\n--- Training Finished ---")

    # --- Final Evaluation ---
    # Load the best model after training for final evaluation
    if best_chk_path_y_prime and os.path.exists(best_chk_path_y_prime):
        print(f"\nLoading best trained model from: {best_chk_path_y_prime} for final evaluation.")
        try:
            checkpoint = torch.load(best_chk_path_y_prime, map_location=DEVICE, weights_only=False)
            generator.load_state_dict(checkpoint['generator_state_dict'])
            detector.load_state_dict(checkpoint['detector_state_dict'])
            print("Best model weights loaded successfully for final evaluation.")
        except Exception as e:
            print(f"Failed to load best checkpoint for final evaluation: {e}. Proceeding with last trained state.")
    else:
        print("\nNo best checkpoint found. Evaluating with the model's final trained state (or initial if training failed/skipped).")

    # Run the robustness evaluation with the final (or best loaded) models
    final_evaluation_results = evaluate_robustness_parallel_pfb(
        generator,
        detector,
        num_eval_batches=50, # Use a sufficient number of batches for robust evaluation
        batch_size_eval=BATCH_SIZE,
        device=DEVICE,
        dataset_loader=val_loader # Use validation data for evaluation
    )

    print("\nFinal Evaluation script finished.")
    print("\n--- Experiment Y-Prime Results ---")
    import json
    print(json.dumps(final_evaluation_results, indent=2))